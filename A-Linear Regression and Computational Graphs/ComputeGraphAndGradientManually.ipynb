{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Graph Manually (Task 1 And Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph drawn successfully at: graph/my_graph\n",
      "Loss Function value when w0=2,w1=3 : [35]\n",
      "Gradient with respect to w0: 39.0\n",
      "Gradient with respect to w1: 11.0\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph # to view graph\n",
    "import numpy as np\n",
    "\n",
    "########################################## STEP 1: CREATE NODES AND CHILDREN ##################################################\n",
    "class Node:\n",
    "    #Creates Node,its children, its operation and connections\n",
    "   \n",
    "    def __init__(self, name, op, parents=None):\n",
    "        self.name = name\n",
    "        self.op = op\n",
    "        self.parents = [] if parents is None else parents #to keep parents of node\n",
    "        self.children = [] #to keep children if any\n",
    "    def inputs(self):\n",
    "        return self.parents\n",
    "    def consumers(self):\n",
    "        return self.children\n",
    "\n",
    "def children(nodes):\n",
    "    #takes node and add as children to the parent node\n",
    "    for node in nodes:\n",
    "        if node.parents is not None:\n",
    "            for p in node.parents:\n",
    "                p.children.append(node)\n",
    "\n",
    "def sortNodes(nodes):\n",
    "    #takes the list of nodes of graph and does sorting for backpropogation \n",
    "    #and returns sorted graph node\n",
    "    # start by collecting input nodes,\n",
    "    parameter_nodes = [n for n in nodes if len(n.parents)==0]\n",
    "    #check if any input nodes\n",
    "    assert(len(parameter_nodes) > 0), 'no input nodes found'\n",
    "    # get nodes, and sort them\n",
    "    unsorted_internal_nodes = [n for n in nodes if n not in parameter_nodes]\n",
    "    internal_nodes = []\n",
    "    # iteratively remove sinks to perform topological sort\n",
    "    while len(unsorted_internal_nodes) > 0:\n",
    "        for n in unsorted_internal_nodes:\n",
    "            # loop through unsorted nodes and check if it has children and is a sink node or not\n",
    "            is_sink = True\n",
    "            for c in n.children:\n",
    "                if c in unsorted_internal_nodes:\n",
    "                    is_sink = False\n",
    "                    break\n",
    "            if is_sink:\n",
    "                unsorted_internal_nodes.remove(n)\n",
    "                internal_nodes.append(n)\n",
    "    # reversing internal nodes as first nodes in the list should be the one that'll be evaluated first\n",
    "    return parameter_nodes, internal_nodes[::-1]\n",
    "\n",
    "########################################## STEP 2: FORWARD PROPOGATION #####################################################\n",
    "def forwardprop(nodes, parameter_table):\n",
    "    #does forward propagation, takes nodes and\n",
    "    # return the activation corresponding to the output node and \n",
    "    #table storing activations for each node in the graph\n",
    "    \n",
    "    # assign inputs values\n",
    "    for name, tensor in parameter_table.items():\n",
    "        nodes[name].op.assign(tensor)\n",
    "    # propagate, add activations in a dictionary\n",
    "    output_table = {}\n",
    "    for name, node in nodes.items():\n",
    "        # collect parents activations\n",
    "        args = [output_table[p.name] for p in node.inputs()]\n",
    "        output_table[node.name] = node.op.forward(*args)\n",
    "   \n",
    "    return output_table[list(nodes.keys())[-1]], output_table\n",
    "\n",
    "########################################## STEP 3: COMPUTE GRADIENTS #####################################################\n",
    "def computeGrad(node, output_table, parameter_grad_table, output_grad_table):\n",
    "    #Computes gradient for each node and returns gradient\n",
    "    #if already computed return the table\n",
    "    if node.name in output_grad_table:\n",
    "        return output_grad_table[node.name]\n",
    "    # list of gradients \n",
    "    consumer_gradients = [] \n",
    "    for consumer_node in node.consumers():\n",
    "        # index of node in its consumer node inputs\n",
    "        parameter_index = consumer_node.inputs().index(node)\n",
    "        #check for nodes in parameter_grad_table that stores gradients at nodes input, \n",
    "        #which may be summed to compute gradients at node outputs\n",
    "        if consumer_node.name in parameter_grad_table:\n",
    "            consumer_gradients.append(parameter_grad_table[consumer_node.name][parameter_index])\n",
    "        else:\n",
    "            consumer_parameter_activations = [output_table[n.name] for n in consumer_node.inputs()]\n",
    "            # compute the gradient at consumer node output by calling recursively\n",
    "            consumer_output_gradient = computeGrad(consumer_node, output_table, parameter_grad_table, output_grad_table)\n",
    "            # based on gradient at consumer node output, compute gradients at its inputs\n",
    "            consumer_parameter_gradients = consumer_node.op.backward(\\\n",
    "                consumer_output_gradient, *consumer_parameter_activations)\n",
    "            # add gradients at consumer node inputs\n",
    "            parameter_grad_table[consumer_node.name] = consumer_parameter_gradients\n",
    "           \n",
    "            #add to dictionary\n",
    "            consumer_gradients.append(consumer_parameter_gradients[parameter_index])\n",
    "    # gradient at node output is the sum of the gradients at its consumer's inputs\n",
    "    consumer_gradients_sum = reduce((lambda x, y: x + y), consumer_gradients)\n",
    "    # cache gradient at node output\n",
    "    output_grad_table[node.name] = consumer_gradients_sum\n",
    "    return consumer_gradients_sum\n",
    "########################################## STEP 4: BACKWARD PROPOGATION #####################################################\n",
    "def backprop(nodes, output_table):\n",
    "    #does backpropogation and takes output_table which is used to store outputs computed during the forward pass\n",
    "    # and returns table that stores gradients for \n",
    "    # stores gradients at nodes output\n",
    "    output_grad_table = {}\n",
    "    # stores gradients at nodes input, they may be summed to compute gradients at node outputs\n",
    "    parameter_grad_table = {} \n",
    "\n",
    "    # gradient at all sinks node is one\n",
    "    sinks = [n for n in nodes if len(n.consumers()) == 0]\n",
    "    for n in sinks:\n",
    "        output_grad_table[n.name] = np.ones(output_table[n.name].shape)\n",
    "    #getting gradient of rest of the nodes\n",
    "    for node in nodes:\n",
    "        computeGrad(node, output_table, parameter_grad_table, output_grad_table)\n",
    "\n",
    "    return output_grad_table\n",
    "\n",
    "####################################### STEP 5: CREATE GRAPH AND CALL NODE CLASS ##############################################\n",
    "class Graph:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        #dictionary of all nodes\n",
    "        self.nodes = {}\n",
    "        self.parameter_nodes = None\n",
    "        # forward propagation table values\n",
    "        self.output_table = None\n",
    "\n",
    "    def add(self, op, name, inputs=None):\n",
    "        # to make sure name is unique\n",
    "        if name in self.nodes:\n",
    "            raise Exception('node name {} already used'.format(name))\n",
    "        # checking if number of inputs provided was right\n",
    "        if not (inputs is None and op.num_inputs() == 0 \\\n",
    "            or inputs is not None and op.num_inputs() == len(inputs)):\n",
    "            raise Exception('Inputs do not match, expected {}, received {}'\\\n",
    "                .format(str(op.num_inputs()), str(0 if inputs is None else len(inputs))))\n",
    "\n",
    "        input_nodes = None\n",
    "        if inputs is not None:\n",
    "            # collect parents based on their name if strings were passed\n",
    "            if len(inputs) > 0 and type(inputs[0]) == str:\n",
    "                input_nodes = [self.nodes[n] for n in inputs]\n",
    "            else:\n",
    "                input_nodes = inputs\n",
    "        node = Node(name, op, input_nodes)\n",
    "        self.nodes[name] = node\n",
    "        return node\n",
    "\n",
    "    def completeGraph(self):\n",
    "        #this function is called once all nodes are added to the graph \n",
    "        #and set the connection of children based on parents and perform the sorting for backpropogation \n",
    "        #calling to add children to parent\n",
    "        children(self.nodes.values())\n",
    "        #sorting all nodes\n",
    "        self.parameter_nodes, internal_nodes = sortNodes(self.nodes.values())\n",
    "        # reordering nodes dictionary\n",
    "        self.nodes = {n.name: n for n in self.parameter_nodes}\n",
    "        for n in internal_nodes:\n",
    "            self.nodes[n.name] = n\n",
    "            \n",
    "####################################### STEP 6: VISUALIZATION OF GRAPH ##############################################\n",
    "    def visualize(self):\n",
    "        #visualizing the graph using GraphViz library\n",
    "        #check if all nodes are set and sorted\n",
    "        assert(self.parameter_nodes is not None), 'Visualization failed.'\n",
    "        g = Digraph('G', filename='graph/'+self.name, format='png')\n",
    "        filepath='graph/'+self.name\n",
    "        #creating node and edges on graph\n",
    "        for node in self.nodes.values():\n",
    "            node_str = node.name if self.output_table is None else '{}, {}'\\\n",
    "                .format(node.name, str(self.output_table[node.name].shape))\n",
    "            g.node(node.name, node_str)\n",
    "        for node in self.nodes.values():\n",
    "            for p in node.parents:\n",
    "                g.edge(p.name, node.name)\n",
    "        #opens default photo viewer to view graph\n",
    "        g.view()\n",
    "        return ('graph drawn successfully at: ' + filepath)\n",
    "\n",
    "    def forward(self, parameter_table):\n",
    "        # make sure a value has been provided for each input\n",
    "        for i in self.parameter_nodes:\n",
    "            if i.name not in parameter_table:\n",
    "                raise Exception('missing value for parameter {}'.format(i.name))\n",
    "        #does forward propogation and storing in output_table for using it later in backpropogation\n",
    "        output, self.output_table = forwardprop(self.nodes, parameter_table)\n",
    "        return output, self.output_table\n",
    "\n",
    "    def backward(self):\n",
    "        #check if output_table is None, if None that means no forward propogation is done and \n",
    "        #hence no back propogation is performed\n",
    "        if self.output_table is None:\n",
    "            raise Exception('no output_table, \\\n",
    "                you may be attempting to backprop without having first performed forward propagation')\n",
    "        return backprop(list(self.nodes.values()), self.output_table)\n",
    "\n",
    "####################################### STEP 7: DEFINE OPERATION ON NODES ##############################################\n",
    "class OpValue:\n",
    "    #to assign values to node in the graph\n",
    "    def num_inputs(self):\n",
    "        return 0\n",
    "    def assign(self, X):\n",
    "        self.X = X\n",
    "    def forward(self):\n",
    "        return self.X\n",
    "\n",
    "#for addition \n",
    "class OpAdd:\n",
    "    def num_inputs(self):\n",
    "        return 2\n",
    "    def forward(self, A, B):\n",
    "        return A + B\n",
    "    #assigning gradients\n",
    "    def backward(self, grad, A, B):\n",
    "        dA = grad\n",
    "        dB = grad\n",
    "        return [dA, dB]\n",
    "#multiplication of two inputs\n",
    "class OpMultiply:\n",
    "    def num_inputs(self):\n",
    "        return 2\n",
    "    def forward(self, A, B):\n",
    "        return np.dot(A, B)\n",
    "    def backward(self, grad, A, B):\n",
    "        dA = np.dot(grad, B.T)\n",
    "        dB = np.dot(A.T, grad)\n",
    "        return [dA, dB]\n",
    "\n",
    "\n",
    "########################################### STEP 8: IMPLEMENTATION ######################################################\n",
    "#create graph\n",
    "g = Graph('my_graph')\n",
    "\n",
    "#create nodes for loss function f=w0^3 *w1 + w0*w1 +2\n",
    "W_0 = g.add(OpValue(), 'W_0') # weight 1\n",
    "W_1 = g.add(OpValue(), 'W_1') # weight 2\n",
    "b_2 = g.add(OpValue(), 'b_2') # bias 2\n",
    "#multiplying w0 thrice to get w0^3\n",
    "Mul = g.add(OpMultiply(), 'Mul', [W_0, W_0])\n",
    "Mul1 = g.add(OpMultiply(), 'Mul1', [Mul, W_0])\n",
    "#ultiplying to get w0^3 *w1\n",
    "Mul2 = g.add(OpMultiply(), 'Mul2', [Mul1, W_1])\n",
    "#multiplying w0*w1\n",
    "Mul3= g.add(OpMultiply(), 'Mul3', [W_0, W_1])\n",
    "#doing Summation to get w0^3 *w1 + w0*w1 +2\n",
    "Add= g.add(OpAdd(), 'Add', [Mul2, Mul3])\n",
    "Add1= g.add(OpAdd(), 'Add1', [Add, W_1])\n",
    "Add2= g.add(OpAdd(), 'Add2', [Add1, b_2])\n",
    "\n",
    "#calling graph method to complete all nodes and sorting of nodes\n",
    "g.completeGraph()\n",
    "#visualizing graph and saving to specified filePath\n",
    "result=g.visualize()\n",
    "print(result)\n",
    "\n",
    "# initialize inputs w0=2,w1=3 and constant 2 (b_2)\n",
    "parameter_table = {\n",
    "    'W_0': np.array([2]),\n",
    "    'W_1': np.array([3]),\n",
    "    'b_2': np.array([2])}\n",
    "########################################### STEP 9: CALL FORWARD PROPOGATION ###################################################\n",
    "# do forward propagation and calculate the loss function\n",
    "output, output_table = g.forward(parameter_table)\n",
    "print('Loss Function value when w0=2,w1=3 :' ,output)\n",
    "########################################### STEP 10: CALL BACK PROPOGATION ####################################################\n",
    "# do backpropogation to compute gradients at all levels\n",
    "grad_table = g.backward()\n",
    "print ('Gradient with respect to w0:', grad_table['W_0'])\n",
    "print ('Gradient with respect to w1:', grad_table['W_1'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Graph Using Tensorflow (Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function value when w0=2,w1=3 : 35\n",
      "graph written successfully at : graph/\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "################################ STEP 1: INITIALIZE VARIABLES AND CONSTANT ###################################################\n",
    "#Compute loss function = (w0^3)*w1 + w0*w1 + w1 + 2\n",
    "# Declaring two weights w0 and w1 with values 2, 3 and constant with value 2\n",
    "w0 = tf.Variable(2, name='w0')\n",
    "w1 = tf.Variable(3, name='w1')\n",
    "c=tf.constant(2, name='const_2')\n",
    "######################################## STEP 2: DEFINE LOSS FUNCTION  #######################################################\n",
    "f=w0* w0*w0*w1 + w0*w1 + w1 + c\n",
    "filepath='graph/'\n",
    "################################ STEP 3: WRITE SUMMMARY FOR VISUALIZATION ###################################################\n",
    "writer=tf.summary.FileWriter(filepath,tf.get_default_graph())\n",
    "#################################### STEP 4: RUN THE SESSION ###############################################################\n",
    "with tf.Session() as sess:\n",
    "    #initializing both the variables\n",
    "    sess.run(w0.initializer)\n",
    "    sess.run(w1.initializer)\n",
    "    #executing the loss function f defined earlier\n",
    "    result=sess.run(f)\n",
    "    print('Loss Function value when w0=2,w1=3 :' ,result)\n",
    "#closing the writer that is used to write graph for visualizing using Tensorboard\n",
    "writer.close()\n",
    "print('graph written successfully at :' ,filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Gradient Using tf.gradients() (Task 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to w0: [39]\n",
      "Gradient with respect to w1: [11]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "################################ STEP 1: INITIALIZE VARIABLES AND CONSTANT ###################################################\n",
    "#Compute loss function = (w0^3)*w1 + w0*w1 + w1 + 2\n",
    "# Declaring two weights w0 and w1 with values 2, 3 and constant with value 2\n",
    "w0 = tf.Variable(2, name='w0')\n",
    "w1 = tf.Variable(3, name='w1')\n",
    "c=tf.constant(2, name='const_2')\n",
    "######################################## STEP 2: DEFINE LOSS FUNCTION  #######################################################\n",
    "f=w0* w0*w0*w1 + w0*w1 + w1 + c\n",
    "######################################## STEP 3: COMPUTE GRADIENTS  #######################################################\n",
    "#get gradient w.r.t w1\n",
    "grad1 = tf.gradients(f, [w1])\n",
    "#get gradient w.r.t w0\n",
    "grad = tf.gradients(f, w0)\n",
    "\n",
    "#################################### STEP 4: RUN THE SESSION ###############################################################\n",
    "sess = tf.Session()\n",
    "#initialize all the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#get gradient w.r.t w0\n",
    "res = sess.run(grad)\n",
    "#get gradient w.r.t w1\n",
    "res1 = sess.run(grad1)\n",
    "print('Gradient with respect to w0:' ,res) \n",
    "print('Gradient with respect to w1:',res1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
