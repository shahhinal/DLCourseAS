{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegressionUsingTensorflowAutodiff.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "-04sjnUO6cyv",
        "colab_type": "code",
        "outputId": "c477ae30-2e3a-450f-d20a-196afadb4aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#allows to run tensorflow code without creating graph\n",
        "tf.enable_eager_execution()\n",
        "################################ STEP 1: LOADING DATASETS ###################################################\n",
        "# Load Dataset\n",
        "boston = load_boston()\n",
        "# Seperate Data into Features and Labels\n",
        "# Features\n",
        "features_df = pd.DataFrame(np.array(boston.data), columns=[boston.feature_names])\n",
        "# Labels\n",
        "labels_df = pd.DataFrame(np.array(boston.target), columns=['labels'])\n",
        "#splitting data into training, testing and validation datasets\n",
        "X_train = features_df[:300]\n",
        "X_val = features_df[301:401]\n",
        "X_test=features_df[402:]\n",
        "\n",
        "y_train = labels_df[:300]\n",
        "y_val = labels_df[301:401]\n",
        "y_test=labels_df[402:]\n",
        "\n",
        "m, n = X_train.shape\n",
        "m1, n1 = X_test.shape\n",
        "m2, n2 = X_val.shape\n",
        "######################################## STEP 2: PREPROCESSING DATA #########################################################\n",
        "# scaling and normalizing data values\n",
        "scaler = StandardScaler()\n",
        "scaled_housing_data = scaler.fit_transform(X_train)\n",
        "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
        "\n",
        "scaled_housing_data1 = scaler.fit_transform(X_test)\n",
        "scaled_housing_data_plus_bias1 = np.c_[np.ones((m1, 1)), scaled_housing_data1]\n",
        "\n",
        "scaled_housing_data2 = scaler.fit_transform(X_val)\n",
        "scaled_housing_data_plus_bias2 = np.c_[np.ones((m2, 1)), scaled_housing_data2]\n",
        "\n",
        "#converting to numpy array\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "print('Training:', X_train.shape)\n",
        "print('Validation:',X_val.shape)\n",
        "print('Testing:',X_test.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (300, 13)\n",
            "Validation: (100, 13)\n",
            "Testing: (104, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tv-HhHKxfoE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "0aed893b-d028-47ed-a598-5cbc67b4a291"
      },
      "cell_type": "code",
      "source": [
        "################################ STEP 3: INITIALIZE VARIABLES AND CONSTANT ###################################################\n",
        "\n",
        "#decalaring number of epochs and learning rate\n",
        "n_epochs = 1000\n",
        "learning_rate = 0.1\n",
        "#declaring constants for training, testing and validation dataset into tensors\n",
        "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
        "XTEST = tf.constant(scaled_housing_data_plus_bias1, dtype=tf.float32, name=\"XTEST\")\n",
        "XVAL = tf.constant(scaled_housing_data_plus_bias2, dtype=tf.float32, name=\"XVAL\")\n",
        "y = tf.constant(y_train.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
        "#declaring variables for weights\n",
        "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
        "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
        "#finding the loss\n",
        "error = y_pred - y\n",
        "#computing mean square error ‚Ñì(ùë§) =1/2ùëÅ (Summation of [ùë°(i) ‚àí ùë¶(ùë•(i))]^2)\n",
        "mse = (tf.reduce_mean(tf.square(error), name=\"mse\"))/2\n",
        "#to compute gradients\n",
        "variables = [theta]\n",
        "#initialize optimizer(used:AdamOptimizer)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "##################################### STEP 4: USE tf.GradientTape() ######################################################\n",
        "for e in range(n_epochs):\n",
        "    # Use tf.GradientTape() to record the gradients of the loss function\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred =tf.matmul(X, theta, name=\"predictions\")\n",
        "        error = y_pred - y\n",
        "        mse = (tf.reduce_mean(tf.square(error), name=\"mse\"))/2\n",
        "    # calculates the gradients of the loss function with respect to weights stored in variables.\n",
        "    grads = tape.gradient(mse, variables)\n",
        "    # updates weights based on gradients.\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
        "   \n",
        "    if e % 100 == 0:\n",
        "         print('Epoch: {0}, Loss: {1}'.format(e, mse))\n",
        "\n",
        "##################################### STEP 5: PRINT LOSS AND GRADIENTS ######################################################\n",
        "y_predT =tf.matmul(X, theta, name=\"predictions1\")\n",
        "errorT = y_predT - y_train\n",
        "# Mean Squared Error for Training \n",
        "mse = (tf.reduce_mean(tf.square(errorT), name=\"mse\"))/2\n",
        "print('Cost for Training Dataset: ',mse)\n",
        "\n",
        "y_pred2 =tf.matmul(XVAL, theta, name=\"predictions1\")\n",
        "error2 = y_pred2 - y_val\n",
        "# Mean Squared Error for Validation \n",
        "mse2 = (tf.reduce_mean(tf.square(error2), name=\"mse2\"))/2\n",
        "print('Cost for Validation Dataset: ',mse2)\n",
        "\n",
        "y_pred1 =tf.matmul(XTEST, theta, name=\"predictions1\")\n",
        "error1 = y_pred1 - y_test\n",
        "# Mean Squared Error for Testing \n",
        "mse1 = (tf.reduce_mean(tf.square(error1), name=\"mse1\"))/2\n",
        "print('Cost for Testing Dataset: ',mse1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 358.7508239746094\n",
            "Epoch: 100, Loss: 129.6981201171875\n",
            "Epoch: 200, Loss: 44.12686538696289\n",
            "Epoch: 300, Loss: 14.303263664245605\n",
            "Epoch: 400, Loss: 6.524878025054932\n",
            "Epoch: 500, Loss: 5.043664932250977\n",
            "Epoch: 600, Loss: 4.839001178741455\n",
            "Epoch: 700, Loss: 4.8184990882873535\n",
            "Epoch: 800, Loss: 4.817017555236816\n",
            "Epoch: 900, Loss: 4.8169403076171875\n",
            "Cost for Training Dataset:  tf.Tensor(4.8169374, shape=(), dtype=float32)\n",
            "Cost for Validation Dataset:  tf.Tensor(60.73302, shape=(), dtype=float32)\n",
            "Cost for Testing Dataset:  tf.Tensor(64.86148, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}